{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sCokFzlnhWPF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/IS Drafts/B. Yuda\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDbGejBBxwrN",
        "outputId": "1f13832a-eb18-406d-ac38-96bbeb9890a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IS Drafts/B. Yuda\n",
            " Attempt5.ipynb         kaggle.json\n",
            "\u001b[0m\u001b[01;34m'Chest CT'\u001b[0m/             lung-and-colon-cancer-histopathological-images.zip\n",
            " \u001b[01;34mCTMedical\u001b[0m/             \u001b[01;34mModels\u001b[0m/\n",
            "'Data Handling.ipynb'   Tensorflow.ipynb\n",
            " \u001b[01;34mIQ-OTHNCCD\u001b[0m/           'training(Tensorflow).ipynb'\n",
            " IQ-OTHNCCD.ipynb       Untitled0.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_ey5zV1XRDF",
        "outputId": "97e2ad6f-8737-4741-d211-c638e61f00f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modules loaded\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import shutil\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "sns.set_style('darkgrid')\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "print ('modules loaded')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def define_paths(dir):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "    folds = os.listdir(dir)\n",
        "    for fold in folds:\n",
        "        foldpath = os.path.join(dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "            filepaths.append(fpath)\n",
        "            labels.append(fold)\n",
        "    return filepaths, labels\n",
        "\n",
        "def define_df(files, classes):\n",
        "    Fseries = pd.Series(files, name= 'filepaths')\n",
        "    Lseries = pd.Series(classes, name='labels')\n",
        "    return pd.concat([Fseries, Lseries], axis= 1)\n",
        "\n",
        "def create_df(tr_dir, val_dir, ts_dir):\n",
        "    # train dataframe\n",
        "    files, classes = define_paths(tr_dir)\n",
        "    train_df = define_df(files, classes)\n",
        "\n",
        "    # validation dataframe\n",
        "    files, classes = define_paths(val_dir)\n",
        "    valid_df = define_df(files, classes)\n",
        "    # test dataframe\n",
        "    files, classes = define_paths(ts_dir)\n",
        "    test_df = define_df(files, classes)\n",
        "    return train_df, valid_df, test_df"
      ],
      "metadata": {
        "id": "dTUgoYC4x7mM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gens(train_df, valid_df, test_df, batch_size):\n",
        "    img_size = (224, 224)\n",
        "    channels = 3\n",
        "    img_shape = (img_size[0], img_size[1], channels)\n",
        "    ts_length = len(test_df)\n",
        "    test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "    test_steps = ts_length // test_batch_size\n",
        "    def scalar(img):\n",
        "        return img\n",
        "    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)\n",
        "    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
        "    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
        "    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
        "    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)\n",
        "    return train_gen, valid_gen, test_gen"
      ],
      "metadata": {
        "id": "By8_aXRZ1u1d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(gen):\n",
        "    g_dict = gen.class_indices        # defines dictionary {'class': index}\n",
        "    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes)\n",
        "    images, labels = next(gen)        # get a batch size samples from the generator\n",
        "    plt.figure(figsize= (20, 20))\n",
        "    length = len(labels)              # length of batch size\n",
        "    sample = min(length, 25)          # check if sample less than 25 images\n",
        "    for i in range(sample):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        image = images[i] / 255       # scales data to range (0 - 255)\n",
        "        plt.imshow(image)\n",
        "        index = np.argmax(labels[i])  # get image index\n",
        "        class_name = classes[index]   # get class of image\n",
        "        plt.title(class_name, color= 'blue', fontsize= 12)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OcWon6S11zQ5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, model, base_model, patience, stop_patience, threshold, factor, dwell, batches, initial_epoch, epochs, ask_epoch):\n",
        "        super(MyCallback, self).__init__()\n",
        "        self.model = model\n",
        "        self.base_model = base_model\n",
        "        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n",
        "        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
        "        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
        "        self.factor = factor # factor by which to reduce the learning rate\n",
        "        self.dwell = dwell\n",
        "        self.batches = batches # number of training batch to runn per epoch\n",
        "        self.initial_epoch = initial_epoch\n",
        "        self.epochs = epochs\n",
        "        self.ask_epoch = ask_epoch\n",
        "        self.ask_epoch_initial = ask_epoch # save this value to restore if restarting training\n",
        "        # callback variables\n",
        "        self.count = 0 # how many times lr has been reduced without improvement\n",
        "        self.stop_count = 0\n",
        "        self.best_epoch = 1   # epoch with the lowest loss\n",
        "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initial learning rate and save it\n",
        "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
        "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
        "        self.best_weights = self.model.get_weights() # set best weights to model's initial weights\n",
        "        self.initial_weights = self.model.get_weights()   # save initial weights if they have to get restored\n",
        "\n",
        "\n",
        "        # Define a function that will run when train begins\n",
        "    def on_train_begin(self, logs= None):\n",
        "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
        "        print(msg)\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_train_end(self, logs= None):\n",
        "        stop_time = time.time()\n",
        "        tr_duration = stop_time - self.start_time\n",
        "        hours = tr_duration // 3600\n",
        "        minutes = (tr_duration - (hours * 3600)) // 60\n",
        "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
        "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
        "        print(msg)\n",
        "        self.model.set_weights(self.best_weights) # set the weights of the model to the best weights\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs= None):\n",
        "        acc = logs.get('accuracy') * 100 # get batch accuracy\n",
        "        loss = logs.get('loss')\n",
        "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
        "        print(msg, '\\r', end= '') # prints over on the same line to show running batch count\n",
        "\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs= None):\n",
        "        self.ep_start = time.time()\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs= None):\n",
        "        ep_end = time.time()\n",
        "        duration = ep_end - self.ep_start\n",
        "\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
        "        current_lr = lr\n",
        "        acc = logs.get('accuracy')  # get training accuracy\n",
        "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
        "        loss = logs.get('loss')  # get training loss for this epoch\n",
        "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
        "\n",
        "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
        "            monitor = 'accuracy'\n",
        "            if epoch == 0:\n",
        "                pimprov = 0.0\n",
        "            else:\n",
        "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
        "\n",
        "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
        "                self.highest_tracc = acc # set new highest training accuracy\n",
        "                self.best_weights = self.model.get_weights() # training accuracy improved so save the weights\n",
        "                self.count = 0 # set count to 0 since training accuracy improved\n",
        "                self.stop_count = 0 # set stop counter to 0\n",
        "                if v_loss < self.lowest_vloss:\n",
        "                    self.lowest_vloss = v_loss\n",
        "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
        "\n",
        "            else:\n",
        "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
        "                # if so adjust learning rate\n",
        "                if self.count >= self.patience - 1: # lr should be adjusted\n",
        "                    lr = lr * self.factor # adjust the learning by factor\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "                    self.count = 0 # reset the count to 0\n",
        "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
        "                    self.count = 0 # reset counter\n",
        "                    if self.dwell:\n",
        "                        self.model.set_weights(self.best_weights) # return to better point in N space\n",
        "                    else:\n",
        "                        if v_loss < self.lowest_vloss:\n",
        "                            self.lowest_vloss = v_loss\n",
        "                else:\n",
        "                    self.count = self.count + 1 # increment patience counter\n",
        "\n",
        "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
        "            monitor = 'val_loss'\n",
        "            if epoch == 0:\n",
        "                pimprov = 0.0\n",
        "            else:\n",
        "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
        "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
        "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
        "                self.best_weights = self.model.get_weights() # validation loss improved so save the weights\n",
        "                self.count = 0 # reset count since validation loss improved\n",
        "                self.stop_count = 0\n",
        "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
        "            else: # validation loss did not improve\n",
        "                if self.count >= self.patience - 1: # need to adjust lr\n",
        "                    lr = lr * self.factor # adjust the learning rate\n",
        "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
        "                    self.count = 0 # reset counter\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "                    if self.dwell:\n",
        "                        self.model.set_weights(self.best_weights) # return to better point in N space\n",
        "                else:\n",
        "                    self.count = self.count + 1 # increment the patience counter\n",
        "                if acc > self.highest_tracc:\n",
        "                    self.highest_tracc = acc\n",
        "\n",
        "\n",
        "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
        "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
        "            print(msg)\n",
        "            self.model.stop_training = True # stop training\n",
        "\n",
        "        else:\n",
        "            if self.ask_epoch != None:\n",
        "                if epoch + 1 >= self.ask_epoch:\n",
        "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
        "                    print(msg)\n",
        "                    ans = input('')\n",
        "                    if ans == 'H' or ans == 'h':\n",
        "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
        "                        print(msg)\n",
        "                        self.model.stop_training = True # stop training\n",
        "                    else:\n",
        "                        try:\n",
        "                            ans = int(ans)\n",
        "                            self.ask_epoch += ans\n",
        "                            msg = f' training will continue until epoch ' + str(self.ask_epoch)\n",
        "                            print(msg)\n",
        "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
        "                            print(msg)\n",
        "                        except:\n",
        "                            print('Invalid')\n"
      ],
      "metadata": {
        "id": "zOSAdqrt11cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training(hist):\n",
        "    tr_acc = hist.history['accuracy']\n",
        "    tr_loss = hist.history['loss']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    index_loss = np.argmin(val_loss)     # get number of epoch with the lowest validation loss\n",
        "    val_lowest = val_loss[index_loss]    # get the loss value of epoch with the lowest validation loss\n",
        "    index_acc = np.argmax(val_acc)       # get number of epoch with the highest validation accuracy\n",
        "    acc_highest = val_acc[index_acc]     # get the loss value of epoch with the highest validation accuracy\n",
        "\n",
        "    plt.figure(figsize= (20, 8))\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    Epochs = [i+1 for i in range(len(tr_acc))]\t       # create x-axis by epochs count\n",
        "    loss_label = f'best epoch= {str(index_loss + 1)}'  # label of lowest val_loss\n",
        "    acc_label = f'best epoch= {str(index_acc + 1)}'    # label of highest val_accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
        "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
        "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
        "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
        "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.tight_layout\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zJFL4pUhhKq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n",
        "    plt.figure(figsize= (10, 10))\n",
        "    plt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation= 45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
        "        print('Normalized Confusion Matrix')\n",
        "    else:\n",
        "        print('Confusion Matrix, Without Normalization')\n",
        "    print(cm)\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')"
      ],
      "metadata": {
        "id": "mXdE3rYLhPxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Structure"
      ],
      "metadata": {
        "id": "KdU6sSGvhSEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Dataframes\n",
        "train_dir = '../input/chest-ctscan-images/Data/train'\n",
        "test_dir = '../input/chest-ctscan-images/Data/valid'\n",
        "valid_dir = '../input/chest-ctscan-images/Data/test'\n",
        "train_df, valid_df, test_df = create_df(train_dir, valid_dir, test_dir)\n",
        "\n",
        "# Get Generators\n",
        "batch_size = 40\n",
        "train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n",
        "\n",
        "show_images(train_gen)"
      ],
      "metadata": {
        "id": "XJAn5ajqhTyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create Pre-trained model"
      ],
      "metadata": {
        "id": "sCokFzlnhWPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
        "\n",
        "# create pre-trained model\n",
        "base_model = tf.keras.applications.efficientnet.EfficientNetB3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
        "    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
        "                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
        "    Dropout(rate= 0.45, seed= 123),\n",
        "    Dense(class_count, activation= 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "gFzF7SCehXWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Get custom callbacks parameters"
      ],
      "metadata": {
        "id": "S4gyp0pvhd1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 40\n",
        "epochs = 100\n",
        "patience = 1 \t\t# number of epochs to wait to adjust lr if monitored value does not improve\n",
        "stop_patience = 3 \t# number of epochs to wait before stopping training if monitored value does not improve\n",
        "threshold = 0.9 \t# if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\n",
        "factor = 0.5 \t\t# factor to reduce lr by\n",
        "dwell = True \t\t# experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\n",
        "freeze = False \t\t# if true free weights of  the base model\n",
        "ask_epoch = 5\t\t# number of epochs to run before asking if you want to halt training\n",
        "batches = int(np.ceil(len(train_gen.labels) / batch_size))\n",
        "\n",
        "callbacks = [MyCallback(model= model, base_model= base_model, patience= patience,\n",
        "            stop_patience= stop_patience, threshold= threshold, factor= factor,\n",
        "            dwell= dwell, batches= batches, initial_epoch= 0, epochs= epochs, ask_epoch= ask_epoch )]"
      ],
      "metadata": {
        "id": "0-wMCDHrhbqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n",
        "                    validation_data= valid_gen, validation_steps= None, shuffle= False,\n",
        "                    initial_epoch= 0)"
      ],
      "metadata": {
        "id": "jSgoDB-Qhk2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(history)"
      ],
      "metadata": {
        "id": "iDUelTl_hprq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_length = len(test_df)\n",
        "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "test_steps = ts_length // test_batch_size\n",
        "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
        "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
        "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
        "\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train Accuracy: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation Accuracy: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])\n"
      ],
      "metadata": {
        "id": "vDVqUex5hses"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict_generator(test_gen)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "FGX02VfXhuKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix and Classification Report"
      ],
      "metadata": {
        "id": "Fiu7bZGQh0xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['adenocarcinoma', 'large-cell-carcinoma', 'normal', 'squamous-cell-carcinoma']\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_gen.classes, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= target_names, title = 'Confusion Matrix')\n",
        "# Classification report\n",
        "print(classification_report(test_gen.classes, y_pred, target_names= target_names))"
      ],
      "metadata": {
        "id": "eSYpTch-hwCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save Model"
      ],
      "metadata": {
        "id": "iFlpOLNZh4AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Effecientnetb3'\n",
        "subject = 'Chest CT-Scan'\n",
        "acc = test_score[1] * 100\n",
        "save_path = './'\n",
        "\n",
        "save_id = str(f'{model_name}-{subject}-{\"%.2f\" %round(acc, 2)}.h5')\n",
        "model_save_loc = os.path.join(save_path, save_id)\n",
        "model.save(model_save_loc)\n",
        "print(f'model was saved as {model_save_loc}')"
      ],
      "metadata": {
        "id": "x6Wa74-hh6qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####"
      ],
      "metadata": {
        "id": "VZocQOEMh7ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict = train_gen.class_indices\n",
        "height = []\n",
        "width = []\n",
        "for _ in range(len(class_dict)):\n",
        "    height.append(img_size[0])\n",
        "    width.append(img_size[1])\n",
        "\n",
        "Index_series = pd.Series(list(class_dict.values()), name= 'class_index')\n",
        "Class_series = pd.Series(list(class_dict.keys()), name= 'class')\n",
        "Height_series = pd.Series(height, name= 'height')\n",
        "Width_series = pd.Series(width, name= 'width')\n",
        "class_df = pd.concat([Index_series, Class_series, Height_series, Width_series], axis= 1)\n",
        "subject = 'Chest CT-Scan'\n",
        "csv_name = f'{subject}-class_dict.csv'\n",
        "csv_save_loc = os.path.join(save_path, csv_name)\n",
        "class_df.to_csv(csv_save_loc, index= False)\n",
        "print(f'class csv file was saved as {csv_save_loc}')"
      ],
      "metadata": {
        "id": "RDmideSKh9sz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}